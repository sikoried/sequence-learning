{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Auto-Correct\n",
    "\n",
    "_See [Assignment 1: Auto-correct](https://sikoried.github.io/sequence-learning/01/autocorrect/)._\n",
    "\n",
    "## String Distances\n",
    "\n",
    "Gemischtes Doppel 1 | Gemischtes Doppel 2 | Gemischtes Doppel 3\n",
    "-|-|-\n",
    "![Gemischtes Doppel 1](res/gem_doppel_1.jpg) | ![Gemischtes Doppel 2](res/gem_doppel_2.jpg) | ![Gemischtes Doppel 3](res/gem_doppel_2.jpg)\n",
    "\n",
    "In the first part of the exercise, we will compute the Hamming and edit distances for the string pairs above (source: Gemischten Doppel, Süddeutsche).\n",
    "Let's start with the simpler one: [Hamming distance](https://en.wikipedia.org/wiki/Hamming_distance).\n",
    "Since this distance is only defined for strings of equal length; for your implementation, make a reasonable modification to support different lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "gem_doppel = [\n",
    "    (\"GCGTATGAGGCTAACGC\", \"GCTATGCGGCTATACGC\"),\n",
    "    (\"kühler schrank\", \"schüler krank\"),\n",
    "    (\"the longest\", \"longest day\"),\n",
    "    (\"nicht ausgeloggt\", \"licht ausgenockt\"),\n",
    "    (\"gurken schaben\", \"schurkengaben\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hamming('GCGTATGAGGCTAACGC', 'GCTATGCGGCTATACGC') = 10\n",
      "hamming('kühler schrank', 'schüler krank') = 13\n",
      "hamming('the longest', 'longest day') = 11\n",
      "hamming('nicht ausgeloggt', 'licht ausgenockt') = 4\n",
      "hamming('gurken schaben', 'schurkengaben') = 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lengths do not match; using approximation\n",
      "lengths do not match; using approximation\n"
     ]
    }
   ],
   "source": [
    "def hamming(x, y):\n",
    "    if len(x) != len(y):\n",
    "        print(\"lengths do not match; using approximation\", file=sys.stderr)\n",
    "\n",
    "    diffs = 0\n",
    "    for i in range(0, min(len(x), len(y))):\n",
    "        if x[i] != y[i]:\n",
    "            diffs += 1\n",
    "\n",
    "    # add the length difference as approximation\n",
    "    return diffs + abs(len(x) - len(y))\n",
    "\n",
    "for (a, b) in gem_doppel:\n",
    "    print(\"hamming('%s', '%s') = %d\" % (a, b, hamming(a, b)))\n",
    "\n",
    "\n",
    "# hamming('GCGTATGAGGCTAACGC', 'GCTATGCGGCTATACGC') = 10\n",
    "# hamming('kühler schrank', 'schüler krank') = 13\n",
    "# hamming('the longest', 'longest day') = 11\n",
    "# hamming('nicht ausgeloggt', 'licht ausgenockt') = 4\n",
    "# hamming('gurken schaben', 'schurkengaben') = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the Hamming distances are quite large, since only viable \"operations\" are _match_ (no cost) and _replace_ (cost 1). For a more nuanced measure, implement the [edit distance](https://en.wikipedia.org/wiki/Edit_distance) also allows for insertions and deletions. Make sure to make the cost of those operations configurable.\n",
    "\n",
    "_Hint:_ This is a good opportunity to familiarize yourself with [`numpy`](https://numpy.org/) and its matrices and range operators; we'll use those throughout the semester."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edit('GCGTATGAGGCTAACGC', 'GCTATGCGGCTATACGC') = 3\n",
      "edit('kühler schrank', 'schüler krank') = 6\n",
      "edit('the longest', 'longest day') = 8\n",
      "edit('nicht ausgeloggt', 'licht ausgenockt') = 4\n",
      "edit('gurken schaben', 'schurkengaben') = 7\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def edit(x, y, cost={'m': 0, 's': 1, 'i': 1, 'd': 1}):\n",
    "    D = np.zeros((len(x) + 1, len(y) + 1), dtype=int)\n",
    "\n",
    "    # for the empty word, costs match the length of the other string\n",
    "    D[0, 1:] = range(1, len(y) + 1)\n",
    "    D[1:, 0] = range(1, len(x) + 1)\n",
    "    \n",
    "    for i in range(1, len(x) + 1):\n",
    "        for j in range(1, len(y) + 1):\n",
    "            delta = cost['m'] if x[i-1] == y[j-1] else cost['s']\n",
    "            D[i, j] = min(\n",
    "                D[i-1, j] + cost['d'],\n",
    "                D[i, j-1] + cost['i'],\n",
    "                D[i-1, j-1] + delta\n",
    "            )\n",
    "\n",
    "    return D[len(x), len(y)]\n",
    "\n",
    "\n",
    "for (a, b) in gem_doppel:\n",
    "    print(\"edit('%s', '%s') = %d\" % (a, b, edit(a, b)))\n",
    "\n",
    "    \n",
    "# edit('GCGTATGAGGCTAACGC', 'GCTATGCGGCTATACGC') = 3\n",
    "# edit('kühler schrank', 'schüler krank') = 6\n",
    "# edit('the longest', 'longest day') = 8\n",
    "# edit('nicht ausgeloggt', 'licht ausgenockt') = 4\n",
    "# edit('gurken schaben', 'schurkengaben') = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the edit distances relate much better to the similarity of the strings, but they still don't really tell us where and how the strings differ.\n",
    "Extend your implementation from above by also computing a backtrace of the operations which can be used to print the alignment of the two strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edit('GCGTATGAGGCTAACGC', 'GCTATGCGGCTATACGC') = 3 (mmdmmmmsmmmmmimmmm)\n",
      "  GCGTATGAGGCTA-ACGC\n",
      "    d    *     i    \n",
      "  GC-TATGCGGCTATACGC\n",
      "edit('kühler schrank', 'schüler krank') = 6 (ssmimmmmsddmmmm)\n",
      "  küh-ler schrank\n",
      "  ** i    *dd    \n",
      "  schüler k--rank\n",
      "edit('the longest', 'longest day') = 8 (ddddmmmmmmmiiii)\n",
      "  the longest----\n",
      "  dddd       iiii\n",
      "  ----longest day\n",
      "edit('nicht ausgeloggt', 'licht ausgenockt') = 4 (smmmmmmmmmmsmssm)\n",
      "  nicht ausgeloggt\n",
      "  *          * ** \n",
      "  licht ausgenockt\n",
      "edit('gurken schaben', 'schurkengaben') = 7 (siimmmmmsdddmmmm)\n",
      "  g--urken schaben\n",
      "  *ii     *ddd    \n",
      "  schurkeng---aben\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "from functools import reduce\n",
    "from numpy import argmin\n",
    "\n",
    "\n",
    "def edit2(x, y, cost={'m': 0, 's': 1, 'i': 1, 'd': 1}):\n",
    "    D = np.zeros((len(x) + 1, len(y) + 1), dtype=int)\n",
    "\n",
    "    # for the empty word, costs match the length of the other string\n",
    "    D[0, 1:] = range(1, len(y) + 1)\n",
    "    D[1:, 0] = range(1, len(x) + 1)\n",
    "\n",
    "    # this array will hold the journal of operations for backtracking\n",
    "    T = np.zeros((len(x) + 1, len(y) + 1), dtype=np.object_)\n",
    "    T[0, 0] = 'ε'\n",
    "    T[0, 1:] = 'i'\n",
    "    T[1:, 0] = 'd'\n",
    "    \n",
    "    for i in range(1, len(x) + 1):\n",
    "        for j in range(1, len(y) + 1):\n",
    "            diag = 'm' if x[i-1] == y[j-1] else 's'\n",
    "            \n",
    "            costs = [\n",
    "                ('d', D[i-1, j] + cost['d']),\n",
    "                ('i', D[i, j-1] + cost['i']),\n",
    "                (diag, D[i-1, j-1] + cost[diag])\n",
    "            ]\n",
    "        \n",
    "            op, c = min(costs, key=operator.itemgetter(1))\n",
    "            D[i, j] = c\n",
    "            T[i, j] = op\n",
    "    \n",
    "    # compute trace\n",
    "    a, b = len(x), len(y)\n",
    "    tr = []\n",
    "    while a > 0 or b > 0:\n",
    "        op = T[a, b]\n",
    "        tr.append(op)\n",
    "        if op == 'm' or op == 's':\n",
    "            a -= 1\n",
    "            b -= 1\n",
    "        elif op == 'd':\n",
    "            a -= 1\n",
    "        elif op == 'i':\n",
    "            b -= 1\n",
    "        else:\n",
    "            raise ValueError('Invalid operator: ' + str(op))\n",
    "    \n",
    "    return D[len(x), len(y)], reduce(operator.add, reversed(tr))\n",
    "\n",
    "\n",
    "for (a, b) in gem_doppel:\n",
    "    d, tr = edit2(a, b)\n",
    "    print(\"edit('%s', '%s') = %d (%s)\" % (a, b, d, tr))\n",
    "\n",
    "    for i, op in enumerate(tr):\n",
    "        if op == 'i':\n",
    "            a = a[:i] + '-' + a[i:]\n",
    "        if op == 'd':\n",
    "            b = b[:i] + '-' + b[i:]\n",
    "\n",
    "    print('  ' + a)\n",
    "    print('  ' + tr.replace('m', ' ').replace('s', '*'))\n",
    "    print('  ' + b)\n",
    "\n",
    "# edit('GCGTATGAGGCTAACGC', 'GCTATGCGGCTATACGC') = 3 (mmdmmmmsmmmmmimmmm)\n",
    "#   GCGTATGAGGCTA-ACGC\n",
    "#     d    *     i    \n",
    "#   GC-TATGCGGCTATACGC\n",
    "# edit('kühler schrank', 'schüler krank') = 6 (ssmimmmmsddmmmm)\n",
    "#   küh-ler schrank\n",
    "#   ** i    *dd    \n",
    "#   schüler k--rank\n",
    "# edit('the longest', 'longest day') = 8 (ddddmmmmmmmiiii)\n",
    "#   the longest----\n",
    "#   dddd       iiii\n",
    "#   ----longest day\n",
    "# edit('nicht ausgeloggt', 'licht ausgenockt') = 4 (smmmmmmmmmmsmssm)\n",
    "#   nicht ausgeloggt\n",
    "#   *          * ** \n",
    "#   licht ausgenockt\n",
    "# edit('gurken schaben', 'schurkengaben') = 7 (siimmmmmsdddmmmm)\n",
    "#   g--urken schaben\n",
    "#   *ii     *ddd    \n",
    "#   schurkeng---aben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling Correction\n",
    "\n",
    "For spelling correction, we will use prior knowledge, to put some learning into our system.\n",
    "The underlying idea is the Noisy Channel Model, that is: The user intends to write a word w, but through some noise in the process, happens to type the word x.\n",
    "\n",
    "The correct word ŵ  is that word, that is a valid candidate and has the highest probability:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\DeclareMathOperator*{\\argmax}{argmax}\n",
    "\\hat{w} & = & \\argmax_{w \\in V} P(w | x) \\\\\n",
    "        & = & \\argmax_{w \\in V} \\frac{P(x|w) P(w)}{P(x)} \\\\\n",
    "        & = & \\argmax_{w \\in V} P(x|w) P(w)\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "- The candidates V can be obtained from a vocabulary.\n",
    "- The probability $P(w)$ of a word w can be learned (counted) from data.\n",
    "- The probability $P(x|w)$ is more complicated... It could be learned from data, but we could also use a heuristic that relates to the edit distance, e.g. rank by distance.\n",
    "\n",
    "You can find word statistics and training data at: <http://norvig.com/ngrams/> (The single word counts are part of this repo).\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- <http://norvig.com/spell-correct.html>\n",
    "- Mays, Eric, Fred J. Damerau and Robert L. Mercer. 1991. Context based spelling correction. _Information Processing and Management,_ 23(5), 517–522. (IBM)\n",
    "- Kernighan, Mark D., Kenneth W. Church, and William A. Gale. 1990. A spelling correction program based on a noisy channel model. _Proceedings of COLING 1990,_ 205-210. (Bell Labs)\n",
    "\n",
    "### Step 1: Read in vocabulary and counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 333333 lemmas.\n"
     ]
    }
   ],
   "source": [
    "from numpy import log\n",
    "\n",
    "# contains lines of \"word <count>\"\n",
    "counts_fn = 'data/count_1w.txt.gz'\n",
    "\n",
    "# read in vocabulary\n",
    "voc = {}\n",
    "all_counts = 0\n",
    "num = 0\n",
    "\n",
    "import gzip\n",
    "\n",
    "with gzip.open(counts_fn, \"rb\") as f:\n",
    "    for line in f:\n",
    "        w, c = line.strip().split()\n",
    "        voc[w.decode('ascii')] = int(c)\n",
    "        all_counts += int(c)\n",
    "        num += 1\n",
    "\n",
    "# normalize the counts\n",
    "for k in voc:\n",
    "    voc[k] = log(voc[k] / all_counts)\n",
    "    \n",
    "print(\"Read in %d lemmas.\" % len(voc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Baseline implementation\n",
    "\n",
    "Implement a (pretty inefficient) spell corrector that, for a given word `w`, suggests at most `max_cand=5` candidate words.\n",
    "To speed up the computation a little bit, consider only words that differ at most `max_dist=3` in length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pirates [('pirates', 0, -11.408058827802126)]\n",
      "pirutes [('pirates', 1, -11.408058827802126), ('minutes', 2, -8.717825438953103), ('viruses', 2, -11.111468702571859)]\n",
      "continoisly [('continously', 1, -15.735337826575178), ('continuously', 2, -11.560071979871001), ('continuosly', 2, -17.009283000138204)]\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "# suggest a list of candidates for the entered word\n",
    "def suggest(w, max_cand=5, max_dist=3):\n",
    "    # if we have an exact hit, just return that.\n",
    "    if w in voc:\n",
    "        return [(w, 0, voc[w])]\n",
    "    \n",
    "    # maps a word to (other, edit-dist, other-rel-freq)\n",
    "    def check(w, kv):\n",
    "        ed = edit(w, kv[0])\n",
    "        return (kv[0], ed, kv[1])\n",
    "    \n",
    "    # compute edit distance of all words that differ at most max_dist in length\n",
    "    res = [check(w, kv) for kv in voc.items() if abs(len(w) - len(kv[0])) < max_dist]\n",
    "    \n",
    "    # now sort descending by relative frequency then ascending by edit distance\n",
    "    res = sorted(res, key=itemgetter(2), reverse=True)\n",
    "    res = sorted(res, key=itemgetter(1))\n",
    "\n",
    "    \n",
    "    return res[:max_cand]\n",
    "\n",
    "\n",
    "examples = [\n",
    "    \"pirates\",    # in-voc\n",
    "    \"pirutes\",    # pirates?\n",
    "    \"continoisly\",  # continuosly?\n",
    "]\n",
    "\n",
    "for w in examples:\n",
    "    print(w, suggest(w, max_cand=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Better Heuristic\n",
    "\n",
    "Let's use a more sophisticated heuristic, that doesn't sort the data twice, but combines the distance with the relative frequency.\n",
    "Here's the gist of it: while the distances are (typically) 0, 1, 2, 3..., the relative frequencies are very small numbers.\n",
    "\n",
    "To model the Bayesian rule above, we need two quantities:\n",
    "\n",
    "- $P(w)$: this is just the relative frequency\n",
    "- $P(x|w)$: let's assume that about 1/3 of the time, we're just one symbol off; 1/2 for two, etc. (These don't really form probabilities, but we just want something probability-like :-)\n",
    "- we may want to balance those quantities, since they might be orders of magnitude different\n",
    "\n",
    "Mathematically speaking, we want something like\n",
    "\n",
    "$$\n",
    "\\hat{w} = \\argmax_{w \\in V} = P(x|w) * P(w)^\\beta \\quad .\n",
    "$$\n",
    "\n",
    "For numerical reasons, we can apply the `log` and discard factors that are equal for all words:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\DeclareMathOperator*{\\argmax}{argmax}\n",
    "\\hat{w} & = & \\argmax_{w \\in V} \\log(P(x|w)) + \\beta \\log(P(w)) \\\\\n",
    "        & = & \\argmax_{w \\in V} \\left[ -\\log(\\frac{1}{2 + \\text{edit}(w, x)}) + \\beta \\log \\frac{\\text{count}(w)}{\\sum_x \\text{count}(x)} \\right] \\\\\n",
    "        & = & \\argmax_{w \\in V} \\left[ \\beta \\log \\text{count}(w) - \\log\\left(2 + \\text{edit}(w, x)\\right) \\right]\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 333333 lemmas.\n"
     ]
    }
   ],
   "source": [
    "# reload the voc since we're now working with counts directly\n",
    "with gzip.open(counts_fn, \"rb\") as f:\n",
    "    for line in f:\n",
    "        w, c = line.strip().split()\n",
    "        voc[w.decode('ascii')] = int(c)\n",
    "\n",
    "print(\"Read %d lemmas.\" % len(voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pirates [('pirates', 0.6931471805599453, 1.569214519355234)]\n",
      "pirutes [('pirates', 1.0986122886681098, 1.569214519355234), ('minutes', 1.3862943611198906, 1.8382378582401362), ('prices', 1.6094379124341003, 1.9310363514927111)]\n",
      "continoisly [('continuously', 1.3862943611198906, 1.5540132041483465), ('continously', 1.0986122886681098, 1.1364866194779288), ('continuity', 1.6094379124341003, 1.561483500710584)]\n"
     ]
    }
   ],
   "source": [
    "def suggest2(w, beta, max_cand=5, max_dist=3, cost={'m': 0, 's': 1, 'i': 1, 'd': 1}):\n",
    "    # if we have an exact hit, just return that.\n",
    "    if w in voc:\n",
    "        return [(w, log(2), beta*log(voc[w]))]\n",
    "    \n",
    "    def check(w, kv):\n",
    "        ed = edit(w, kv[0])\n",
    "        return (kv[0], log(2 + ed), beta*log(kv[1]))\n",
    "\n",
    "    res = [check(w, kv) for kv in voc.items() if abs(len(w) - len(kv[0])) < max_dist]\n",
    "    res = sorted(res, key=lambda x: x[2] - x[1], reverse=True)\n",
    "    return res[:max_cand]\n",
    "  \n",
    "\n",
    "examples = [\n",
    "    \"pirates\",    # in-voc\n",
    "    \"pirutes\",    # pirates?\n",
    "    \"continoisly\",  # continuosly?\n",
    "]\n",
    "\n",
    "for w in examples:\n",
    "    print(w, suggest2(w, 0.1, max_cand=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient Implementation\n",
    "\n",
    "Use a prefix tree to efficiently compute the edit distance for a large number of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 333333 words\n"
     ]
    }
   ],
   "source": [
    "# reload the voc since we're now working with counts directly\n",
    "\n",
    "class PrefixTree:    \n",
    "    voc = dict()\n",
    "    def __init__(self, parent, prefix, count=None):\n",
    "        self.parent = parent\n",
    "        self.prefix = prefix\n",
    "        self.count = count\n",
    "        self.succ = dict()\n",
    "        self.hypref = None\n",
    "    \n",
    "    def size(self):\n",
    "        return len(voc)\n",
    "    \n",
    "    def clean_hyprefs(self):\n",
    "        agenda = [self]\n",
    "        while agenda:\n",
    "            n = agenda.pop()\n",
    "            n.hypref = {}\n",
    "            agenda.extend([s for (k, s) in n.succ.items()])    \n",
    "    \n",
    "    def insert(self, word, count):\n",
    "        it = self\n",
    "        for (i, c) in enumerate(word):\n",
    "            if not c in it.succ:\n",
    "                it.succ[c] = PrefixTree(it, word[:i+1])\n",
    "            it = it.succ[c]\n",
    "        if it.count:\n",
    "            raise ValueError(\"%s already in tree\" % word)\n",
    "        it.count = count\n",
    "        self.voc[word] = count\n",
    "\n",
    "    # query the score of a word in the tree\n",
    "    def query(self, word):\n",
    "        it = self\n",
    "        for i in word:\n",
    "            if not i in it.succ:\n",
    "                raise ValueError(\"%s not found\" % word)\n",
    "            it = it.succ[i]\n",
    "        return it.count\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str({'prefix': self.prefix, 'succ': list(self.succ.keys()), 'count': self.count})\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.prefix if self.prefix else 'None'\n",
    "\n",
    "    def to_string_lines(self):\n",
    "        res = []\n",
    "        agenda = [(n, 1) for (k, n) in sorted(self.succ.items(), reverse=True)]\n",
    "        while agenda:\n",
    "            n, d = agenda.pop()\n",
    "            if not n.count:\n",
    "                res.append(\"%s %s\" % (' '*d, n.prefix))\n",
    "            else:\n",
    "                res.append(\"%s %s %d\" % (' '*d, n.prefix, n.count))\n",
    "                \n",
    "            for (k, s) in sorted(n.succ.items(), reverse=True):\n",
    "                agenda.append((s, d+1))\n",
    "        \n",
    "        return res\n",
    "\n",
    "    \n",
    "\n",
    "# read all entries into the prefix tree\n",
    "root = PrefixTree(None, 'ε')\n",
    "\n",
    "# populate with some words\n",
    "# for (w1, w2) in gem_doppel:\n",
    "#     root.insert(w1, 1)\n",
    "#     root.insert(w2, 1)\n",
    "    \n",
    "\n",
    "# tiny example\n",
    "# root.insert('haus', 1)\n",
    "# root.insert('habe', 1)\n",
    "# root.insert('hau', 1)\n",
    "# root.insert('auto', 1)\n",
    "# root.insert('autark', 1)\n",
    "\n",
    "# root.insert('toupi', 1)\n",
    "\n",
    "# root.insert('pirates', 1)\n",
    "# root.insert('pirutes', 1)\n",
    "\n",
    "import gzip\n",
    "with gzip.open(counts_fn, \"rb\") as f:\n",
    "    for line in f:\n",
    "        w, c = line.strip().split()\n",
    "        root.insert(w.decode('ascii'), int(c))\n",
    "\n",
    "# print('\\n'.join(root.to_string_lines()))\n",
    "\n",
    "# print(root.query('pirate'))\n",
    "# print(root.query('continuous'))\n",
    "# print(root.query('continous'))  # lol.\n",
    "\n",
    "print(\"Indexed %d words\" % len(root.voc.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "# efficient implementation of edit distance for larg vocabulary\n",
    "def edit3(root, w, max_dist=3, cost={'m': 0, 's': 1, 'i': 1, 'd': 1}):\n",
    "    # effectively, we'll build a shadow tree with refs to the original nodes \n",
    "    # initial, eps-row in D\n",
    "    eps = {\n",
    "        'token': 'ε',\n",
    "        'noderef': root,\n",
    "        'backref': None,\n",
    "        'succ': dict(),\n",
    "        'D': list(range(len(w)+1))\n",
    "    }\n",
    "    \n",
    "    # populate the eps-cols; use items to be able to sort by key\n",
    "    agenda = [(eps, 0)]\n",
    "    while agenda:\n",
    "        cur, depth = agenda.pop()\n",
    "    \n",
    "        for (c, n) in sorted(cur['noderef'].succ.items()):\n",
    "            node = {\n",
    "                'token': c,\n",
    "                'noderef': n,\n",
    "                'backref': cur,\n",
    "                'succ': dict(),\n",
    "                'D': [depth+1]\n",
    "            }\n",
    "            cur['succ'][c] = node\n",
    "            agenda.append((node, depth+1))\n",
    "    \n",
    "    # we'll do a depth-first search, one char at a time\n",
    "    eds = {}\n",
    "    for (j, c) in enumerate(w, start=1):\n",
    "        # start at depth=1\n",
    "        agenda = [(t, sn, 1) for (t, sn) in sorted(eps['succ'].items())]\n",
    "        while agenda:\n",
    "            token, shadow_node, depth = agenda.pop()\n",
    "            delta = cost['m'] if c == token else cost['s']\n",
    "            \n",
    "            # costs for each step\n",
    "            cost_del = shadow_node['backref']['D'][j] + cost['d']  # D[i-1, j] one letter \"up\" = backref!\n",
    "            cost_ins = shadow_node['D'][j-1] + cost['i']           # D[i, j-1] one letter \"left\" = same line\n",
    "            cost_dia = shadow_node['backref']['D'][j-1] + delta    # D[i-1, j-1] one up+left\n",
    "\n",
    "            # ...decide\n",
    "            step = min(cost_del, cost_ins, cost_dia)\n",
    "        \n",
    "            shadow_node['D'].append(step)\n",
    "            \n",
    "            agenda.extend([(t, sn, depth+1) for (t, sn) in sorted(shadow_node['succ'].items())])                \n",
    "            \n",
    "            # at the end of the input word, if we have a word, update the edit distances accordingly\n",
    "            if j == len(w) and step < max_dist:\n",
    "                n = shadow_node['noderef']\n",
    "                if n.count:\n",
    "                    eds[n.prefix] = step  # (step, n.count)\n",
    "            \n",
    "    return eds\n",
    "\n",
    "print(edit3(root, 'pirutes'))\n",
    "\n",
    "\n",
    "# compare timings\n",
    "# for e in examples[0:1]:\n",
    "#     print(e)\n",
    "    \n",
    "#     #%time individual = [(w, edit(e, w)) for w in root.voc]\n",
    "#     #print(sorted(individual.items(), key=itemgetter(1))[:10])\n",
    "    \n",
    "#     %time prefixed = edit3(root, e)\n",
    "#     print(sorted(prefixed.items(), key=itemgetter(1))[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pirates [('tomates', 3), ('timated', 3), ('timates', 2), ('ticktes', 3)]\n",
      "pirutes [('timates', 3), ('ticktes', 3), ('tirades', 3), ('airlies', 3)]\n",
      "continoisly [('continuosly', 2), ('continually', 3), ('continously', 1), ('continuously', 2)]\n"
     ]
    }
   ],
   "source": [
    "class Hyp:\n",
    "    _cost = {'m': 0, 's': 1, 'i': 1, 'd': 1}\n",
    "    def __init__(self, d, j, noderef):\n",
    "        self.d = d  # depth (=row number)\n",
    "        self.j = j  # character offset (=col number)\n",
    "        self.noderef = noderef\n",
    "        \n",
    "        self.c = -1\n",
    "        # some cost can be found right there\n",
    "        if d == 0 and j == 0:\n",
    "            self.c = 0\n",
    "        elif d == 0:\n",
    "            self.c = j * Hyp._cost['i']\n",
    "        elif j == 0:\n",
    "            self.c = d * Hyp._cost['d']\n",
    "        \n",
    "        # back-refs\n",
    "        self.refi = None\n",
    "        self.refd = None\n",
    "        self.refs = None\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"(%s, %d, %d, %s)\" % (self.noderef.prefix, self.d, self.j, self.c)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.d == other.d and self.j == other.j and self.noderef == other.noderef\n",
    "\n",
    "    def cost(self, word):\n",
    "        if self.c < 0:\n",
    "            vals = []\n",
    "            if self.refs:\n",
    "                delta = Hyp._cost['m'] if self.noderef.prefix[-1] == word[self.j-1] else Hyp._cost['s']\n",
    "                vals.append(self.refs.cost(word) + delta)\n",
    "            if self.refd:\n",
    "                vals.append(self.refd.cost(word) + Hyp._cost['d'])\n",
    "            if self.refi:\n",
    "                vals.append(self.refi.cost(word) + Hyp._cost['i'])\n",
    "            if not vals:\n",
    "                raise ValueError(\"Unable to obtain cost; check agenda sorting! %s %s\" % (str(self), word))\n",
    "\n",
    "            self.c = min(vals)\n",
    "\n",
    "        return self.c\n",
    "\n",
    "    \n",
    "def edit4(root, w, max_cost=3):\n",
    "    # clear out any hypref in the tree\n",
    "    root.clean_hyprefs()\n",
    "    \n",
    "    # edit distances to return\n",
    "    eds = {}\n",
    "    \n",
    "    # initial hypothesis\n",
    "    eps = Hyp(0, 0, root)\n",
    "    root.hypref[0] = eps\n",
    "    \n",
    "    agenda = [eps]\n",
    "    while agenda:\n",
    "        h = agenda.pop(0)\n",
    "    \n",
    "        # print(h.cost(w), h)\n",
    "        \n",
    "        if h.cost(w) > max_cost:\n",
    "            # print(\"Abandoning %s\" % h)\n",
    "            continue\n",
    "            \n",
    "        if h.j == len(w):\n",
    "            n = h.noderef\n",
    "            if n.count:\n",
    "                eds[n.prefix] = h.cost(w)\n",
    "        \n",
    "        # don't expand longer than the word\n",
    "        if h.j in h.noderef.hypref:\n",
    "            i = h.noderef.hypref[h.j]\n",
    "            i.refi = h\n",
    "            # print(\"~%s.refi = %s\" %(i, h))\n",
    "        else:\n",
    "            if h.j < len(w) and h.j * Hyp._cost['i'] < max_cost:\n",
    "                i = Hyp(h.d, h.j+1, h.noderef)\n",
    "                i.refi = h\n",
    "                if h.refd:\n",
    "                    i.refs = h.refd\n",
    "                agenda.append(i)\n",
    "                h.noderef.hypref[h.j] = i\n",
    "                # print(\"+i %s\" % i)\n",
    "        \n",
    "        # only expand \"downwards\" if there's more successors (depth)\n",
    "        for (t, n) in h.noderef.succ.items():\n",
    "            if h.j in n.hypref:\n",
    "                # get(n.prefix, h.d+1, h.j)\n",
    "                d = n.hypref[h.j]\n",
    "                d.refd = h\n",
    "                # print(\"~%s.refd = %s\" %(d, h))\n",
    "            else:\n",
    "                if h.d * Hyp._cost['d'] < max_cost:\n",
    "                    d = Hyp(h.d+1, h.j, n)\n",
    "                    d.refd = h\n",
    "                    if h.refi:\n",
    "                        d.refs = h.refi\n",
    "                    agenda.append(d)\n",
    "                    n.hypref[h.j] = d\n",
    "                    # print(\"+d %s\" % d)\n",
    "        \n",
    "            if (h.j+1) in n.hypref:\n",
    "                # get(n.prefix, h.d+1, h.j+1)\n",
    "                s = n.hypref[h.j+1]\n",
    "                s.refs = h\n",
    "                # print(\"~%s.refs = %s\" %(s, h))\n",
    "            else:\n",
    "                if h.j < len(w):\n",
    "                    s = Hyp(h.d+1, h.j+1, n)\n",
    "                    s.refs = h\n",
    "                    agenda.append(s)\n",
    "                    n.hypref[h.j+1] = s\n",
    "                    # print(\"+s %s\" % s)\n",
    "    \n",
    "    return eds\n",
    "\n",
    "\n",
    "# for e in ['hans']:\n",
    "for e in examples:\n",
    "    print(e, list(edit4(root, e).items())[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pirates\n",
      "CPU times: user 32.5 s, sys: 380 ms, total: 32.9 s\n",
      "Wall time: 36 s\n",
      "CPU times: user 13.9 s, sys: 259 ms, total: 14.1 s\n",
      "Wall time: 14.3 s\n",
      "CPU times: user 18 s, sys: 154 ms, total: 18.1 s\n",
      "Wall time: 18.2 s\n",
      "pirutes\n",
      "CPU times: user 30.4 s, sys: 165 ms, total: 30.6 s\n",
      "Wall time: 31.1 s\n",
      "CPU times: user 13.9 s, sys: 211 ms, total: 14.1 s\n",
      "Wall time: 14.3 s\n",
      "CPU times: user 17.6 s, sys: 128 ms, total: 17.8 s\n",
      "Wall time: 17.9 s\n",
      "continoisly\n",
      "CPU times: user 45.4 s, sys: 347 ms, total: 45.7 s\n",
      "Wall time: 45.8 s\n",
      "CPU times: user 18.9 s, sys: 223 ms, total: 19.1 s\n",
      "Wall time: 19.4 s\n",
      "CPU times: user 18.6 s, sys: 117 ms, total: 18.7 s\n",
      "Wall time: 18.8 s\n"
     ]
    }
   ],
   "source": [
    "# final benchmark\n",
    "for e in examples:\n",
    "    print(e)\n",
    "    \n",
    "    # old-school\n",
    "    %time individual = [(w, edit(e, w)) for w in root.voc]\n",
    "    \n",
    "    # matrix in prefix tree\n",
    "    %time prefixed = edit3(root, e)\n",
    "\n",
    "    # sort-of-beamsearch\n",
    "    %time beamed = edit4(root, e)\n",
    "\n",
    "    #print(sorted(individual.items(), key=itemgetter(1))[:10])\n",
    "    #print(sorted(prefixed.items(), key=itemgetter(1))[:10])\n",
    "    #print(sorted(beamed.items(), key=itemgetter(1))[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

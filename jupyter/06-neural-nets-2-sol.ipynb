{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:0.4786653324407056\n",
      "Error:0.005203530307080479\n",
      "Error:0.003565026697301017\n",
      "Error:0.0028639142087672938\n",
      "Error:0.002453762623741133\n",
      "Error:0.002177404926592433\n",
      "[[-2.82348101 -3.00805313  1.05110743]\n",
      " [ 3.10530313  2.90010145 -1.10334771]\n",
      " [-4.24280279 -4.25676723  1.82666428]\n",
      " [ 1.27133051  1.37549678  0.24622966]] [[-4.32434991  5.72051757 -7.08458511  2.84299564]]\n",
      "[[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [0 0 1]]\n",
      "[[0 1 1 0]]\n",
      "[[0.00185871 0.99791783 0.99789823 0.00185871]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def nonlin(x,deriv=False):\n",
    "    if deriv:\n",
    "        return x*(1-x)\n",
    "    else:\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "    \n",
    "# data\n",
    "X = np.array([[0,0,1],\n",
    "            [0,1,1],\n",
    "            [1,0,1],\n",
    "            [0,0,1]])\n",
    "                \n",
    "y = np.array([[0],\n",
    "            [1],\n",
    "            [1],\n",
    "            [0]])\n",
    "\n",
    "\n",
    "# seed the NRG for reproducible results\n",
    "np.random.seed(1)\n",
    "\n",
    "# randomly initialize our weights with mean 0\n",
    "#layer0 = 2 * np.random.rand(4, 3) - 1  # mean weight 0\n",
    "#layer1 = 2 * np.random.rand(1, 4) - 1\n",
    "\n",
    "# for exact compatibility with the original script nnet2_orig.py\n",
    "# to maintain the same rand() calls\n",
    "# but then save transposeds.\n",
    "layer0 = 2 * np.random.rand(3, 4) - 1  # mean weight 0\n",
    "layer1 = 2 * np.random.rand(4, 1) - 1\n",
    "\n",
    "layer0 = layer0.T\n",
    "layer1 = layer1.T\n",
    "\n",
    "learning_rate = 1\n",
    "\n",
    "for j in range(0, 60000):\n",
    "\n",
    "    # Feed forward through layers 0, 1, and 2\n",
    "    l0 = X.T\n",
    "    l1 = nonlin(np.matmul(layer0, l0))\n",
    "    l2 = nonlin(np.matmul(layer1, l1))\n",
    "\n",
    "    # how much did we miss the target value?\n",
    "    l2_error = y.T - l2\n",
    "    \n",
    "    if (j% 10000) == 0:\n",
    "        print(\"Error:\" + str(np.mean(np.abs(l2_error))))\n",
    "        \n",
    "    # in what direction is the target value?\n",
    "    # were we really sure? if so, don't change too much.\n",
    "    l2_delta = l2_error * nonlin(l2, deriv=True)\n",
    "\n",
    "    # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
    "    l1_error = np.matmul(layer1.T, l2_delta)\n",
    "    \n",
    "    # in what direction is the target l1?\n",
    "    # were we really sure? if so, don't change too much.\n",
    "    l1_delta = l1_error * nonlin(l1, deriv=True)\n",
    "\n",
    "    # sum up the partial contributions by each sample to form\n",
    "    # the final gradient\n",
    "    layer1 += (learning_rate * np.matmul(l2_delta, l1.T))\n",
    "    layer0 += (learning_rate * np.matmul(l1_delta, l0.T))\n",
    "\n",
    "print(layer0, layer1)\n",
    "\n",
    "l0 = X.T\n",
    "l1 = nonlin(np.matmul(layer0, l0))\n",
    "l2 = nonlin(np.matmul(layer1, l1))\n",
    "\n",
    "print(X)\n",
    "print(y.T)\n",
    "print(l2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

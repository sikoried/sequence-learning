{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Auto-Correct\n",
    "\n",
    "_See [Assignment 1: Auto-correct](https://sikoried.github.io/sequence-learning/01/autocorrect/)._\n",
    "\n",
    "## String Distances\n",
    "\n",
    "Gemischtes Doppel 1 | Gemischtes Doppel 2 | Gemischtes Doppel 3\n",
    "-|-|-\n",
    "![Gemischtes Doppel 1](res/gem_doppel_1.jpg) | ![Gemischtes Doppel 2](res/gem_doppel_2.jpg) | ![Gemischtes Doppel 3](res/gem_doppel_2.jpg)\n",
    "\n",
    "In the first part of the exercise, we will compute the Hamming and edit distances for the string pairs above (source: Gemischten Doppel, Süddeutsche).\n",
    "Let's start with the simpler one: [Hamming distance](https://en.wikipedia.org/wiki/Hamming_distance).\n",
    "Since this distance is only defined for strings of equal length; for your implementation, make a reasonable modification to support different lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "gem_doppel = [\n",
    "    (\"GCGTATGAGGCTAACGC\", \"GCTATGCGGCTATACGC\"),\n",
    "    (\"kühler schrank\", \"schüler krank\"),\n",
    "    (\"the longest\", \"longest day\"),\n",
    "    (\"nicht ausgeloggt\", \"licht ausgenockt\"),\n",
    "    (\"gurken schaben\", \"schurkengaben\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming(x, y):\n",
    "    # TODO compute the hamming distance\n",
    "    return 0\n",
    "\n",
    "for (a, b) in gem_doppel:\n",
    "    print(\"hamming('%s', '%s') = %d\" % (a, b, hamming(a, b)))\n",
    "\n",
    "\n",
    "# hamming('GCGTATGAGGCTAACGC', 'GCTATGCGGCTATACGC') = 10\n",
    "# hamming('kühler schrank', 'schüler krank') = 13\n",
    "# hamming('the longest', 'longest day') = 11\n",
    "# hamming('nicht ausgeloggt', 'licht ausgenockt') = 4\n",
    "# hamming('gurken schaben', 'schurkengaben') = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the Hamming distances are quite large, since only viable \"operations\" are _match_ (no cost) and _replace_ (cost 1). For a more nuanced measure, implement the [edit distance](https://en.wikipedia.org/wiki/Edit_distance) also allows for insertions and deletions. Make sure to make the cost of those operations configurable.\n",
    "\n",
    "_Hint:_ This is a good opportunity to familiarize yourself with [`numpy`](https://numpy.org/) and its matrices and range operators; we'll use those throughout the semester."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def edit(x, y, cost={'m': 0, 's': 1, 'i': 1, 'd': 1}):\n",
    "    # TODO compute the edit distance with respect to the given costs\n",
    "    return 0\n",
    "\n",
    "\n",
    "for (a, b) in gem_doppel:\n",
    "    print(\"edit('%s', '%s') = %d\" % (a, b, edit(a, b)))\n",
    "\n",
    "    \n",
    "# edit('GCGTATGAGGCTAACGC', 'GCTATGCGGCTATACGC') = 3\n",
    "# edit('kühler schrank', 'schüler krank') = 6\n",
    "# edit('the longest', 'longest day') = 8\n",
    "# edit('nicht ausgeloggt', 'licht ausgenockt') = 4\n",
    "# edit('gurken schaben', 'schurkengaben') = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the edit distances relate much better to the similarity of the strings, but they still don't really tell us where and how the strings differ.\n",
    "Extend your implementation from above by also computing a backtrace of the operations which can be used to print the alignment of the two strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit2(x, y, cost={'m': 0, 's': 1, 'i': 1, 'd': 1}):\n",
    "    D = np.zeros((len(x) + 1, len(y) + 1), dtype=int)\n",
    "\n",
    "    # for the empty word, costs match the length of the other string\n",
    "    D[0, 1:] = range(1, len(y) + 1)\n",
    "    D[1:, 0] = range(1, len(x) + 1)\n",
    "\n",
    "    # this array will hold the journal of operations for backtracking\n",
    "    T = np.zeros((len(x) + 1, len(y) + 1), dtype=np.object_)\n",
    "    T[0, 0] = 'ε'\n",
    "    T[0, 1:] = 'i'\n",
    "    T[1:, 0] = 'd'\n",
    "    \n",
    "    # compute the trace from T and return the alignment string\n",
    "    return D[len(x), len(y)], '???'\n",
    "\n",
    "\n",
    "# test it, output some useful visualizations of the alignment strings\n",
    "for (a, b) in gem_doppel:\n",
    "    d, tr = edit2(a, b)\n",
    "    print(\"edit('%s', '%s') = %d (%s)\" % (a, b, d, tr))\n",
    "\n",
    "    for i, op in enumerate(tr):\n",
    "        if op == 'i':\n",
    "            a = a[:i] + '-' + a[i:]\n",
    "        if op == 'd':\n",
    "            b = b[:i] + '-' + b[i:]\n",
    "\n",
    "    print('  ' + a)\n",
    "    print('  ' + tr.replace('m', ' ').replace('s', '*'))\n",
    "    print('  ' + b)\n",
    "\n",
    "# edit('GCGTATGAGGCTAACGC', 'GCTATGCGGCTATACGC') = 3 (mmdmmmmsmmmmmimmmm)\n",
    "#   GCGTATGAGGCTA-ACGC\n",
    "#     d    *     i    \n",
    "#   GC-TATGCGGCTATACGC\n",
    "# edit('kühler schrank', 'schüler krank') = 6 (ssmimmmmsddmmmm)\n",
    "#   küh-ler schrank\n",
    "#   ** i    *dd    \n",
    "#   schüler k--rank\n",
    "# edit('the longest', 'longest day') = 8 (ddddmmmmmmmiiii)\n",
    "#   the longest----\n",
    "#   dddd       iiii\n",
    "#   ----longest day\n",
    "# edit('nicht ausgeloggt', 'licht ausgenockt') = 4 (smmmmmmmmmmsmssm)\n",
    "#   nicht ausgeloggt\n",
    "#   *          * ** \n",
    "#   licht ausgenockt\n",
    "# edit('gurken schaben', 'schurkengaben') = 7 (siimmmmmsdddmmmm)\n",
    "#   g--urken schaben\n",
    "#   *ii     *ddd    \n",
    "#   schurkeng---aben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling Correction\n",
    "\n",
    "For spelling correction, we will use prior knowledge, to put some learning into our system.\n",
    "The underlying idea is the Noisy Channel Model, that is: The user intends to write a word w, but through some noise in the process, happens to type the word x.\n",
    "\n",
    "The correct word ŵ  is that word, that is a valid candidate and has the highest probability:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\DeclareMathOperator*{\\argmax}{argmax}\n",
    "\\hat{w} & = & \\argmax_{w \\in V} P(w | x) \\\\\n",
    "        & = & \\argmax_{w \\in V} \\frac{P(x|w) P(w)}{P(x)} \\\\\n",
    "        & = & \\argmax_{w \\in V} P(x|w) P(w)\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "- The candidates V can be obtained from a vocabulary.\n",
    "- The probability $P(w)$ of a word w can be learned (counted) from data.\n",
    "- The probability $P(x|w)$ is more complicated... It could be learned from data, but we could also use a heuristic that relates to the edit distance, e.g. rank by distance.\n",
    "\n",
    "You can find word statistics and training data at: <http://norvig.com/ngrams/> (The single word counts are part of this repo).\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- <http://norvig.com/spell-correct.html>\n",
    "- Mays, Eric, Fred J. Damerau and Robert L. Mercer. 1991. Context based spelling correction. _Information Processing and Management,_ 23(5), 517–522. (IBM)\n",
    "- Kernighan, Mark D., Kenneth W. Church, and William A. Gale. 1990. A spelling correction program based on a noisy channel model. _Proceedings of COLING 1990,_ 205-210. (Bell Labs)\n",
    "\n",
    "### Step 1: Read in vocabulary and counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contains lines of \"word <count>\"\n",
    "counts_fn = 'data/count_1w.txt.gz'\n",
    "\n",
    "# read in vocabulary: str -> count\n",
    "voc = {}\n",
    "\n",
    "import gzip\n",
    "with gzip.open(counts_fn, \"rb\") as f:\n",
    "    for line in f:\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "for (k, v) in voc:\n",
    "    # TODO normalize the counts\n",
    "    pass\n",
    "\n",
    "print(\"Read in %d lemmas.\" % len(voc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Baseline implementation\n",
    "\n",
    "Implement a (pretty inefficient) spell corrector that, for a given word `w`, suggests at most `max_cand=5` candidate words.\n",
    "To speed up the computation a little bit, consider only words that differ at most `max_dist=3` in length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "# suggest a list of candidates for the entered word\n",
    "def suggest(w, max_cand=5, max_dist=3):\n",
    "    # TODO\n",
    "    # nb: if the word is an exact hit in the vocab, return just that word\n",
    "\n",
    "\n",
    "    # make sure the list is just max_cand long...\n",
    "    return []\n",
    "\n",
    "\n",
    "examples = [\n",
    "    \"pirates\",      # in-voc\n",
    "    \"pirutes\",      # pirates?\n",
    "    \"continoisly\",  # continuosly?\n",
    "]\n",
    "\n",
    "for w in examples:\n",
    "    print(w, suggest(w, max_cand=3))\n",
    "    \n",
    "\n",
    "# pirates [('pirates', 0, -11.408058827802126)]\n",
    "# pirutes [('pirates', 1, -11.408058827802126), ('minutes', 2, -8.717825438953103), ('viruses', 2, -11.111468702571859)]\n",
    "# continoisly [('continously', 1, -15.735337826575178), ('continuously', 2, -11.560071979871001), ('continuosly', 2, -17.009283000138204)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Better Heuristic\n",
    "\n",
    "Let's use a more sophisticated heuristic, that doesn't sort the data twice, but combines the distance with the relative frequency.\n",
    "Here's the gist of it: while the distances are (typically) 0, 1, 2, 3..., the relative frequencies are very small numbers.\n",
    "\n",
    "To model the Bayesian rule above, we need two quantities:\n",
    "\n",
    "- $P(w)$: this is just the relative frequency\n",
    "- $P(x|w)$: let's assume that about 1/3 of the time, we're just one symbol off; 1/2 for two, etc. (These don't really form probabilities, but we just want something probability-like :-)\n",
    "- we may want to balance those quantities, since they might be orders of magnitude different\n",
    "\n",
    "Mathematically speaking, we want something like\n",
    "\n",
    "$$\n",
    "\\hat{w} = \\argmax_{w \\in V} = P(x|w) * P(w)^\\beta \\quad .\n",
    "$$\n",
    "\n",
    "For numerical reasons, we can apply the `log` and discard factors that are equal for all words:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\DeclareMathOperator*{\\argmax}{argmax}\n",
    "\\hat{w} & = & \\argmax_{w \\in V} \\log(P(x|w)) + \\beta \\log(P(w)) \\\\\n",
    "        & = & \\argmax_{w \\in V} \\left[ -\\log(\\frac{1}{2 + \\text{edit}(w, x)}) + \\beta \\log \\frac{\\text{count}(w)}{\\sum_x \\text{count}(x)} \\right] \\\\\n",
    "        & = & \\argmax_{w \\in V} \\left[ \\beta \\log \\text{count}(w) - \\log\\left(2 + \\text{edit}(w, x)\\right) \\right]\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO reload the voc if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest2(w, beta, max_cand=5, max_dist=3, cost={'m': 0, 's': 1, 'i': 1, 'd': 1}):\n",
    "    # suggest at most max_cand words, using the above heuristig\n",
    "    return []\n",
    "  \n",
    "\n",
    "examples = [\n",
    "    \"pirates\",    # in-voc\n",
    "    \"pirutes\",    # pirates?\n",
    "    \"continoisly\",  # continuosly?\n",
    "]\n",
    "\n",
    "for w in examples:\n",
    "    print(w, suggest2(w, 0.1, max_cand=3))\n",
    "\n",
    "# pirates [('pirates', 0.6931471805599453, 1.569214519355234)]\n",
    "# pirutes [('pirates', 1.0986122886681098, 1.569214519355234), ('minutes', 1.3862943611198906, 1.8382378582401362), ('prices', 1.6094379124341003, 1.9310363514927111)]\n",
    "# continoisly [('continuously', 1.3862943611198906, 1.5540132041483465), ('continously', 1.0986122886681098, 1.1364866194779288), ('continuity', 1.6094379124341003, 1.561483500710584)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient Implementation\n",
    "\n",
    "Use a prefix tree to efficiently compute the edit distance for a large number of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrefixTree:\n",
    "    # TODO write a basic prefix tree implementation\n",
    "    # nb: you may want to add __str__, __repr__ and a to_string_lines method to debug it...\n",
    "    def __init__(self, parent, prefix):\n",
    "        pass\n",
    "    \n",
    "    def size(self):\n",
    "        pass\n",
    "    \n",
    "    def insert(self, word):\n",
    "        pass\n",
    "\n",
    "    def query(self, word):\n",
    "        pass\n",
    "\n",
    "\n",
    "# read all entries into the prefix tree\n",
    "root = PrefixTree(None, 'ε')\n",
    "\n",
    "# TODO read vocab and store in prefix tree\n",
    "\n",
    "# you may want to debug this...\n",
    "root.insert('haus', 1)\n",
    "root.insert('habe', 1)\n",
    "root.insert('hau', 1)\n",
    "root.insert('auto', 1)\n",
    "root.insert('autark', 1)\n",
    "\n",
    "print(\"Indexed %d words\" % root.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit3(root, word, max_dist=3, cost={'m': 0, 's': 1, 'i': 1, 'd': 1}):\n",
    "    # efficient implementation of edit distance for larg vocabulary stored in orefix tree\n",
    "    # nb: return hits as dict: word -> edit-dist\n",
    "    return {}\n",
    "\n",
    "\n",
    "print(edit3, 'hans')\n",
    "\n",
    "# {haus: 1, habe: 2, ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark your examples\n",
    "for e in examples:\n",
    "    print(e)\n",
    "    \n",
    "    # old-school\n",
    "    %time base = [(w, edit(e, w)) for w in root.voc]\n",
    "    \n",
    "    # faster implementation\n",
    "    %time faster = edit3(root, e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
